<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>About — Trung Quan Cao</title>
  <!-- Preload Tailwind fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  <!-- Tailwind CSS via CDN -->
  <script src="https://cdn.tailwindcss.com"></script>
  <style>
    :root { color-scheme: light dark; }
    html { scroll-behavior: smooth; }
    body { font-family: Inter, ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial, "Apple Color Emoji", "Segoe UI Emoji"; }
    /* al-folio-ish link underline */
    .link { position: relative; }
    .link:after { content: ""; position: absolute; left: 0; bottom: -2px; height: 2px; width: 100%; transform: scaleX(0); transform-origin: left; transition: transform .2s ease; }
    .link:hover:after { transform: scaleX(1); }
    .link { color: rgb(37 99 235); }
    .link:after { background: rgb(37 99 235); }
    @media (prefers-color-scheme: dark) {
      .link { color: rgb(96 165 250); }
      .link:after { background: rgb(96 165 250); }
    }
    /* ABS button custom styling removed due to unsupported @apply; button styles moved inline */
  </style>
</head>

<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Trung Quan Cao</title>
  <!-- Preload Tailwind fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  <!-- Tailwind CSS via CDN -->
  <script src="https://cdn.tailwindcss.com"></script>

  <!-- Favicon: 32x32 black rectangle -->
  <link rel="icon" type="image/svg+xml"
        href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 32 32'><rect width='32' height='32' fill='black'/></svg>">

  <style>
    :root { color-scheme: light dark; }
    html { scroll-behavior: smooth; }
    body { font-family: Inter, ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial, 'Apple Color Emoji', 'Segoe UI Emoji'; }
    /* additional styles ... */
  </style>
</head>
  
<body class="bg-white text-zinc-800 dark:bg-zinc-950 dark:text-zinc-100">
  <!-- Top Nav (static; only About is active) -->
  <header class="sticky top-0 z-40 border-b border-zinc-200/80 dark:border-zinc-800/80 backdrop-blur bg-white/75 dark:bg-zinc-950/60">
    <div class="mx-auto max-w-4xl px-4">
      <div class="flex items-center gap-4 py-3">
        <a href="#" class="text-lg font-semib old"></a>
        <nav class="ml-auto flex items-center gap-1 text-sm">
          <a class="px-3 py-1.5 rounded-md font-medium bg-zinc-900 text-white dark:bg-zinc-100 dark:text-zinc-900" href="#">About</a>
          <!-- <span class="px-3 py-1.5 rounded-md text-zinc-500">Publications</span> -->
          <span class="px-3 py-1.5 rounded-md text-zinc-500">CV</span>
        </nav>
      </div>
    </div>
  </header>

  <!-- Main -->
  <main class="mx-auto max-w-4xl px-4 py-10">
    <!-- Introduction with image -->
    <section class="mb-8 flex flex-col sm:flex-row items-center gap-6">
      <div>
        <h1 class="text-3xl sm:text-4xl font-extrabold tracking-tight">Trung Quan Cao</h1>
        <p class="mt-2 text-base sm:text-lg text-zinc-600 dark:text-zinc-400">
          Bachelor of Science in Artificial Intelligence applicant for Fall - 2026
        </p>
        <div class="mt-4 prose prose-zinc dark:prose-invert max-w-none">
          <p>
            I am an ambitious candidate with award-winner at ISEF, ViSEF and
            national informatics contests. I passionate about mathematics, physics and artificial intelligence. 
          </p>
          
          <p class="mt-2">
            I am seeking an opportunity to advance my research and academic excellence.
          </p>
        </div>
      </div>
      <img src="quancao.png" alt="Trung Quan Cao" class="w-40 h-40 sm:w-48 sm:h-48 rounded-lg object-cover">
    </section>

    <!-- Projects -->
    <section class="mt-10">
      <h2 class="text-xl font-semibold">Projects</h2>
      <ul role="list" class="mt-4 space-y-4">
        <!-- Project 1 -->
        <li class="border border-zinc-200 dark:border-zinc-800 rounded-xl p-4">
          <div class="flex flex-col sm:flex-row sm:items-start gap-4">
            <img src="wheelchair.png" alt="Project thumbnail" class="w-full sm:w-40 h-28 object-cover rounded-lg">
            <div class="flex-1">
              <h3 class="font-semibold">Autonomous Wheelchair for Mobility and Communication Assistance for ALS Patients</h3>
              <p class="text-sm text-zinc-600 dark:text-zinc-400">Quan Cao, Hieu Le</p>
              <p class="mt-2 text-sm text-zinc-700 dark:text-zinc-300">4th Prize at the International Science and Engineering Fair, 2025</p>
              <div class="mt-2 flex flex-wrap gap-3 text-sm">
                <button type="button"
                  class="inline-flex items-center px-2 py-1 text-xs font-semibold border border-blue-600 text-blue-600 dark:border-blue-400 dark:text-blue-400 rounded transition-colors duration-150 hover:bg-blue-50 dark:hover:bg-blue-800"
                  onclick="toggleAbstract('abs1')">ABS</button>
              </div>
              <div id="abs1" class="mt-2 hidden text-sm text-zinc-700 dark:text-zinc-300">
                <p>
                  Amyotrophic Lateral Sclerosis (ALS) patients with severe motor and speech impairments often rely on eye movements and assistive technology to move and communicate. However, current smart wheelchairs are unable to simultaneously address both mobility and communication needs. This project introduces a novel autonomous wheelchair system that integrates advanced technologies to tackle these two challenges, specifically tailored to the unique needs of ALS patients. Firstly, the patient interacts on a laptop interface via pupil movements by a novel eye-tracking subsystem utilizing YOLO11 for pupil detection and polynomial regression for gaze estimation, enabling virtual mouse control. Secondly, the system offers patients the choice between two functions: mobility and communication. For mobility, the system provides two options: manual control, which emulates joystick signals for eight directions, and an autonomous mode. The autonomous mode employs the SLAM Toolbox for mapping and Nav2 for localization, path planning, and navigation with an adjusted DWB controller to enable automatic reverse maneuvers in confined spaces. For communication, the system combines an eye-operated virtual keyboard with a fine-tuned Gemma2 language model that converts discrete word selections into 3 distinct sentences, supporting both speech output and Telegram messaging. Additionally, a user-friendly GUI application integrates all functions on the laptop interface. Finally, testing demonstrated 97.11% accuracy in the gaze point processing, 0.12 meters average path-following deviation, 93.33% success in autonomous navigation, and sentence generation in under 5.33 seconds. This solution represents the first fully integrated mobility-communication system engineered for ALS patients.
                </p>
              </div>
            </div>
          </div>
        </li>
        <!-- Project 2 -->
        <li class="border border-zinc-200 dark:border-zinc-800 rounded-xl p-4">
          <div class="flex flex-col sm:flex-row sm:items-start gap-4">
            <img src="gloves.png" alt="Project thumbnail" class="w-full sm:w-40 h-28 object-cover rounded-lg">
            <div class="flex-1">
              <h3 class="font-semibold">Translational Gloves for Enhancing Communication of Individuals with Speech and Hearing Impairments</h3>
              <p class="text-sm text-zinc-600 dark:text-zinc-400">Quan Cao</p>
              <p class="mt-2 text-sm text-zinc-700 dark:text-zinc-300">3rd Prize at the National Youth Informatics Contest, 2024</p>
              <div class="mt-2 flex flex-wrap gap-3 text-sm">
                <button type="button"
                  class="inline-flex items-center px-2 py-1 text-xs font-semibold border border-blue-600 text-blue-600 dark:border-blue-400 dark:text-blue-400 rounded transition-colors duration-150 hover:bg-blue-50 dark:hover:bg-blue-800"
                  onclick="toggleAbstract('abs2')">ABS</button>
              </div>
              <div id="abs2" class="mt-2 hidden text-sm text-zinc-700 dark:text-zinc-300">
                <p>
                  Individuals with hearing and speech impairments often rely on sign language, yet most translation systems only operate one‑way and output sequences of characters that can be hard to interpret. I developed a two‑way bilingual glove‑based device that not only translates sign language into speech but also converts spoken language back to text, enabling interactive communication. Each glove contains twelve inertial measurement units mounted on the fingers and palm, capturing six axes of acceleration and angular data to build a gesture dataset. A neural network with several hidden layers processes the 7,400 input features to classify gestures with accuracies above 99%. Recognised signs are converted to speech via a gTTS‑based module, while a speech‑recognition module turns voice responses into text for the user. To overcome the fragmentation of sign language, an integrated Gemma2:2B language model combines recognised discrete signs into natural sentences, enhancing comprehension and reducing gesture complexity. The device supports both Vietnamese and English, with potential for additional languages. A custom data augmentation pipeline and self‑training mechanism allow users to add new gestures with minimal data collection, continually expanding the vocabulary. Experiments achieved over 99% accuracy; despite limited datasets and errors from augmentation, accuracy remains acceptable. This solution bridges communication gaps by combining high‑accuracy gesture recognition, natural language generation, bilingual output and extensibility for future gestures.
                </p>
              </div>
            </div>
          </div>
        </li>
        <!-- Project 3 -->
        <li class="border border-zinc-200 dark:border-zinc-800 rounded-xl p-4">
          <div class="flex flex-col sm:flex-row sm:items-start gap-4">
            <img src="flood.png" alt="Project thumbnail" class="w-full sm:w-40 h-28 object-cover rounded-lg">
            <div class="flex-1">
              <h3 class="font-semibold">Applying Machine Learning to Flood Forecasting in Central Vietnam</h3>
              <p class="text-sm text-zinc-600 dark:text-zinc-400">Quan Cao, Khang Nguyen, Sinh Vo, Dang Trinh, Thu Nguyen, Thao Nguyen</p>
              <p class="mt-2 text-sm text-zinc-700 dark:text-zinc-300">Summer in Engineering and Applied Sciences, 2025</p>
              <div class="mt-2 flex flex-wrap gap-3 text-sm">
                <button type="button"
                  class="inline-flex items-center px-2 py-1 text-xs font-semibold border border-blue-600 text-blue-600 dark:border-blue-400 dark:text-blue-400 rounded transition-colors duration-150 hover:bg-blue-50 dark:hover:bg-blue-800"
                  onclick="toggleAbstract('abs3')">ABS</button>
              </div>
              <div id="abs3" class="mt-2 hidden text-sm text-zinc-700 dark:text-zinc-300">
                <p>
                  Aiming to reduce the devastating impacts of seasonal floods in central Vietnam, our team developed a data-driven forecasting system that leverages machine learning to predict river flows in ungauged basins. Because national streamflow datasets are scarce and often incomplete, the project turned to the U.S. CAMELS dataset, selecting gauge stations whose climatic and catchment characteristics resemble those in Vietnam. After preprocessing and normalizing more than 110,000 samples of precipitation, temperature, humidity, solar radiation, day length, terrain and other hydrological features, the team trained a multilayer perceptron (MLP) model to map inputs to streamflow rates. The model was designed to capture the nonlinear relationships between inputs and outputs and optimized via gradient descent; robust scaling mitigated seasonal noise in the data. Validation experiments showed that the MLP generalizes well from the CAMELS training set to the Long Dai River in Quang Binh province, achieving high accuracy and effectively learning flow patterns despite the absence of local streamflow measurements. The system has been deployed as a web-based application that provides real-time flood forecasts for Long Dai, offering an early warning tool for communities in at-risk areas. Future improvements include incorporating satellite imagery, soil moisture and water-level data to enhance predictive performance, expanding coverage to additional river basins, and integrating SMS or app-based alerts so residents can receive timely warnings. The project demonstrates that combining international hydrological datasets with machine learning and web technologies can yield practical flood‑forecasting tools for data‑limited regions.
                </p>
              </div>
            </div>
          </div>
        </li>
      </ul>
    </section>

    <!-- Footer -->
    <footer class="mt-12 text-xs text-zinc-500 dark:text-zinc-400">
      <p>© <span id="y"></span> Trung Quan Cao. Hosted by GitHub Pages.</p>
    </footer>
  </main>

  <script>
    // Year
    document.getElementById('y').textContent = new Date().getFullYear();
    // Toggle abstract visibility
    function toggleAbstract(id) {
      const el = document.getElementById(id);
      if (el.classList.contains('hidden')) {
        el.classList.remove('hidden');
      } else {
        el.classList.add('hidden');
      }
    }
  </script>
</body>
</html>
